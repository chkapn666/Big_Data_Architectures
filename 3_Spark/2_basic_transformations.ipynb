{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31a0ab8-4a49-4f0c-a8da-8601575d3f55",
   "metadata": {},
   "source": [
    "**Definition:** RDD Transformations are operations on RDDs that result into new RDDs.  \n",
    "No updates happen to the existing RDDs (they are by default immutable).  \n",
    "Spark retains RDD lineage, using an operator graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee9457-eaed-4ea0-9671-616016cb61ae",
   "metadata": {},
   "source": [
    "RDD Transformations are lazy operations.  \n",
    "We can distinguish them to:\n",
    "- Narrow transformations: they compute results that live in the same partition as the original RDD. They impose no data movement between partitions. Examples are 'map' and 'filter'.\n",
    "- Wider transformations: they compute results that live in many partitions. Data movement between partitions is possible. Examples are 'groupByKey' and 'reduceByKey'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b821f84-a61a-43b3-b639-36ec65823ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/25 15:09:04 WARN Utils: Your hostname, ChristoorossAir resolves to a loopback address: 127.0.0.1; using 192.168.1.18 instead (on interface en0)\n",
      "25/03/25 15:09:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 15:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/25 15:09:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/25 15:09:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "os.chdir('/Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3_Spark')\n",
    "\n",
    "# For some reason i need to run this every time in order to get it work\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home\" \n",
    "\n",
    "sc = SparkContext(\"local[1]\", \"app\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f051f74-e51d-4168-b59f-8b3588c4a0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic Transformations\n",
    "nums = sc.parallelize([1,2,3])\n",
    "\n",
    "# Pass each element through a function\n",
    "squares = nums.map(lambda x: x * x)  # => [1, 4, 9]\n",
    "\n",
    "# Keep elements passing a predicate\n",
    "even = squares.filter(lambda x: x % 2 == 0)  # => [4]\n",
    "\n",
    "# Map each element to zero or more others\n",
    "nums.flatMap(lambda x: range(0, x))   # => [0,0,1,0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf384c8-0b00-4155-b5f9-93f9ee6ed808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RDD Filtering - \".filter()\" filters an RDD based on a given predicate\n",
    "rdd = sc.parallelize(range(20))\n",
    "rdd1 = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61eedc7d-e592-42c4-b3cc-3ff1f7536ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, True), (1, False), (2, True), (3, False), (4, True), (5, False), (6, True), (7, False), (8, True), (9, False), (10, True), (11, False), (12, True), (13, False), (14, True), (15, False), (16, True), (17, False), (18, True), (19, False)]\n"
     ]
    }
   ],
   "source": [
    "# RDD Map - \".map()\" maps a function to each element of an RDD\n",
    "rdd2 = rdd.map(lambda x: (x, x % 2 == 0))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969b715a-44ce-49a2-a12e-a30f68f994e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5])\n",
    "sq = nums.map(lambda x: x * x).collect()\n",
    "for n in sq:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc316ca-c440-43bf-bea7-665d34fc149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 2, 1], [6, 5, 4], [9, 8, 7]]\n",
      "[3, 2, 1, 6, 5, 4, 9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "# RDD flatMap - \".flatMap()\" maps a function to each element of the RDD and flattens the RDD\n",
    "rdd = sc.parallelize([[1,2,3], [4,5,6], [7,8,9]])  # it will take in each sub-list as an rdd, and will create an rdd made of these 3 sub-rdds\n",
    "rdd1 = rdd.map(lambda x: x[-1::-1])  # so it will revert the current ordering of elements into each sub-list / sub-rdd\n",
    "print(rdd1.collect())\n",
    "rdd2 = rdd.flatMap(lambda x: x[-1::-1])\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e9577-2a20-4e85-a186-36dee64eb1bd",
   "metadata": {},
   "source": [
    "More Transformations:\n",
    "- **map(func)**: returns a new distributed dataset (aka an rdd) by passing each element of the source through a function func\n",
    "- **filter(func)**: returns a new dff formed by selecting those elements of the source on which func returns true\n",
    "- **flatMap(func)**: similar to map, but each input item can be mapped to 0 or more output items\n",
    "- **union(otherDataset)**: return a new rdd that contains the union of the elements in the source dataset and the argument\n",
    "- **intersection(otherDataset)**: returns a new rdd that contains the intersection of elements in the source dataset and the argument\n",
    "- **groupByKey([numPartitions])**: when called on a dataset of (k,v) pairs, returns a dataset of (K, iterable<V>) pairs\n",
    "- **reduceByKey(func, [numPartitions])**: when called on a dataset of (K,V) pairs, returns a dataset of (K,V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V, V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "- **join(otherDataset, [numPartitions])**: when called on datasets of type (K,V) and (K,W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7e77a-07f7-423f-8dae-237a64cddbbd",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "- Definition: they are operations that return the raw values\n",
    "- Any RDD method that returns anything other than an RDD is an RDD Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a73715-069a-43c6-a5fb-78f35c59f69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic Actions\n",
    "nums = sc.parallelize([1,2,3])\n",
    "\n",
    "# Retrieve RDD contents as a local collection\n",
    "nums.collect()  # => [1,2,3]\n",
    "\n",
    "# Returns an ARRAY of the first first K elements\n",
    "nums.take(2)  # => [1,2]\n",
    "\n",
    "# Count number of elements\n",
    "nums.count()  # => 3\n",
    "\n",
    "# Merge elements with an associative function\n",
    "nums.reduce(lambda x, y: x+y)  # => 6\n",
    "\n",
    "# Write elements to a text file\n",
    "nums.saveAsTextFile('file:///' + os.getcwd() + '/file.txt')  # or 'hdfs:///file.txt' if i did not want a local copy but one on hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd045c-34c0-4a4b-a635-f2d939ba6d4e",
   "metadata": {},
   "source": [
    "Apache Spark's fold(), reduce(), and aggregate() operations provide flexible ways   \n",
    "to process distributed data in RDDs. Letâ€™s analyze their behavior and use cases.  \n",
    "\n",
    "## RDD Fold Operation\n",
    "The fold() action aggregates elements within partitions first, then combines partition results.\n",
    "### Key characteristics:\n",
    "- Requires a neutral element (zeroValue) that serves as:\n",
    "- Initial value for partition-level aggregation\n",
    "- Initial value for combining partition results\n",
    "- **Guarantees a result even for empty RDDs**\n",
    "- **Must use associative operations**\n",
    "\n",
    "Example breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78249b3d-e885-4343-9683-a8e75938b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,3,6,8,3,4,6,7], 3)  # 3 partitions minimum (as a hint; NOT guaranteed!)\n",
    "sum = rdd.fold(0, lambda x,y: x+y)  # Neutral element = 0 (identity for addition)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e9aff-7430-4ddf-854b-59a1c350599d",
   "metadata": {},
   "source": [
    "Execution logic:\n",
    "- Partition 1: 0 + 1 = 1\n",
    "- Partition 2: 0 + 3 + 6 = 9\n",
    "- Partition 3: 0 + 8 + 3 + 4 + 6 + 7 = 28\n",
    "- Combine results: 0 + (1) + (9) + (28) = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f018a357-c183-4e7c-8be3-2f831a4ccd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1, Max: 8, Sum: 38\n"
     ]
    }
   ],
   "source": [
    "# RDD Fold - another example\n",
    "rdd = sc.parallelize([1,3,6,8,3,4,6,7])  # no # partitions specified - it will be automatically assigned the number of cores on our local machine \n",
    "sum = rdd.fold(0, lambda x, y: x + y)  # zero is the neutral element of summation - it does NOT affect the sum \n",
    "\n",
    "# For the following two fold operations, I am going to provide (at random) the first element of each partition as the initial value \n",
    "# I do not give it float('inf') or None, because spark applies the zeroValue to every partition, so the zeroValue must be of the SAME TYPE as the DATA\n",
    "# So if we did zeroValue=None, it would raise a TypeError because None and int cannot be compared with min()\n",
    "minv = rdd.fold(rdd.take(1)[0], lambda x, y: min(x, y))  # .take(1) returns an array with a single element, so we do .take(1)[0] to retrieve the element itself\n",
    "maxv = rdd.fold(rdd.take(1)[0], lambda x, y: max(x, y))\n",
    "\n",
    "print(f\"Min: {minv}, Max: {maxv}, Sum: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c8460fa-94ea-448b-b484-e219a69a56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1, Max: 8, Sum: 38\n"
     ]
    }
   ],
   "source": [
    "# We could perform the exact same action using the '.reduce()' method\n",
    "\n",
    "rdd = sc.parallelize([1,3,6,8,3,4,6,7])\n",
    "sum = rdd.reduce(lambda x, y: x + y)  # No initial value !!! \n",
    "minv = rdd.reduce(lambda x, y: min(x, y))\n",
    "maxv = rdd.reduce(lambda x, y: max(x, y))\n",
    "\n",
    "print(f\"Min: {minv}, Max: {maxv}, Sum: {sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebed68-28bd-4657-8f8a-755e54b86804",
   "metadata": {},
   "source": [
    "## ðŸ“Œ .reduce()  \n",
    "- **Pros:** More efficient when neutral element isn't needed\n",
    "- **Cons:** Fails on empty RDDs\n",
    "- **Requires** associative and commutative operations\n",
    "#### If the RDD we try to digest is empty, .reduce() will throw an error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c98040-b1a0-441e-b7f4-6c8ef6de0242",
   "metadata": {},
   "source": [
    "## ðŸ“Œ .fold()  \n",
    "Use when:\n",
    "- Handling empty RDDs (returns zeroValue) \n",
    "- Custom initial states are needed \n",
    "- Can also work with non-commutative operations \n",
    "\n",
    "**If the RDD we try to digest is empty, .fold() will not throw an error; it will return the specified initial value**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dfc7437-e782-4b6c-92b4-2e87259a8dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[41] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key-value operations\n",
    "\n",
    "pets = sc.parallelize([('cat', 1), ('dog', 1), ('cat', 2)])\n",
    "\n",
    "pets.reduceByKey(lambda x, y: x+y)\n",
    "# => [('cat',3), ('dog', 1)]\n",
    "\n",
    "pets.groupByKey()\n",
    "# => [('cat', [1,2]), ('dog', [1])]\n",
    "\n",
    "pets.sortByKey()  # by default, the underlying iterators (when the single elements of the RDD are iterators) are compared based on their first element\n",
    "# and if that is the same, then by the second element, etc \n",
    "# => [('cat', 1), ('cat', 2), ('dog', 1)]\n",
    "# Moreover, rdd.sortByKey() by default sets 'ascending=True'!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a91252-bf31-47d8-a0b7-bd9c76929846",
   "metadata": {},
   "source": [
    "### RDD Aggregate - \".aggregate()\" operates on RDD partitions \n",
    "It **removes .fold()'s type constraint** by allowing:  \n",
    "- Different input/output types  \n",
    "- Separate functions for partition aggregation (seqOp) and result merging (combOp)  \n",
    "\n",
    "The aggregate function first aggregates elements in each partition and then aggregates results of all partitions to get the final result (like the reducebykey or fold).  \n",
    "It takes 3 parameters:\n",
    "- An initial value for the aggregation of each partition\n",
    "- A function that takes two parameters of the type of the RDD elements AND returns a result of POSSIBLY DIFFERENT TYPE\n",
    "- A function that takes two parameters of the type of the result of the RDD aggregations and returns the final result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cede953-3f48-426f-84db-38cbe6bc90f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 25\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,4,5,2,4,6,1])  # when we do not define the number of partitions, it automatically becomes equal to the number of cpu cores on our machine\n",
    "# here we will compute the sum of element values across all partitions\n",
    "r = rdd.aggregate(\n",
    "    0,   # Neutral element of addition\n",
    "    (lambda acc, v: acc + v),   # seqOp (partition aggregation) -> arguments: accumulator, value\n",
    "    (lambda acc1, acc2: acc1+acc2)    # combOp (result merging)  -> arguments: accumulator1, accumulator2 - the fact that we use 2 accumulators has nothing to do \n",
    "    # with using 2 partitions - it is just the standard way of expressing this associative operation\n",
    ")\n",
    "\n",
    "print(f\"Output {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91c379f5-d9ca-4008-bd12-d2a957670a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world use case: Calculating average with sum and count:   [the FINAL result of .aggregate() will comprise of 2 values => returned in a tuple]\n",
    "(sum_val, count) = rdd.aggregate(\n",
    "    (0,0),  # one of the initialization values will represent the running sum of the values across partitions, and the other will represent a running count of the values we come across\n",
    "    lambda acc,v: (acc[0]+v, acc[1]+1),  # Update sum and count per element\n",
    "    lambda acc1,acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])  # Combine results\n",
    ")\n",
    "\n",
    "avg = sum_val / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61162914-972a-4b94-b226-c6511144baba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.125\n"
     ]
    }
   ],
   "source": [
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd0ad28-6358-4417-8160-d0d1c03d3df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 190\n"
     ]
    }
   ],
   "source": [
    "# Another aggregate example\n",
    "rdd = sc.parallelize([(\"A\",10),(\"B\",20),(\"B\",30),(\"C\",40),(\"D\",30),(\"E\",60)])\n",
    "r = rdd.aggregate(\n",
    "    0,\n",
    "    lambda acc, v: acc + v[1],\n",
    "    lambda acc1, acc2: acc1 + acc2\n",
    ")\n",
    "print(f\"Output {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47dd6da6-e6aa-4cfa-8d9f-93ec47404401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Element 0\n",
      "Element 1\n",
      "Element 2\n",
      "Element 3\n",
      "Element 4\n"
     ]
    }
   ],
   "source": [
    "# RDD ForEach\n",
    "\n",
    "rdd = sc.parallelize(range(5))\n",
    "rdd.foreach(lambda x: print('Element {0}'.format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00445fc5-58d9-44e9-b54e-d19dedceecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD saveAsTextFile\n",
    "# ATTENTION: it expects it in a full path preceded by \"file:\" to accept it\n",
    "rdd.saveAsTextFile('file:/Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3_Spark/out.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3615baa-bf27-4b96-851d-1eb9f3e90242",
   "metadata": {},
   "source": [
    "### Further RDD Actions\n",
    "- **reduce(func)**: Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n",
    "- **collect()**: return all the elements of the dataset as an array at the driver program. This is typically useful after a filter or other operation that returns a sufficiently small subset of the data.\n",
    "- **count():** return the number of elements in the dataset\n",
    "- **first():** return the first element of the dataset (similar to take(1)[0[)\n",
    "- **take(n):** return **AN ARRAY** with the first n elements of the dataset\n",
    "- **saveAsTextFile(path)**: write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\n",
    "- **countByKey()**: returns a dictionary with keys the values of the dataset and values the number of their occurrences\n",
    "- **foreach(func)**: run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59120d2c-997e-448c-956a-405370c4ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Example: Word Count\n",
    "lines = sc.textFile('file:///' + os.getcwd() + '/hamlet.txt')  # !!! this is the way we need to input files in order for the sparkContext.textFile() method to work\n",
    "counts = lines.flatMap(lambda line: line.split(' ')) \\\n",
    "            .map(lambda word: (word, 1)) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .sortBy(lambda kv: kv[1], ascending=False) \\\n",
    "            .take(10)#.collect()\n",
    "\n",
    "# so the previous takes in the input file, \n",
    "# breaks each line into a (flat) list of words,\n",
    "# calculates the total count of occurrences for each word,\n",
    "# sorts (word, total_count) pairs by the total_count value in ascending order,\n",
    "# shows the words with the top 10 total_count values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f59d6a-18f0-4c51-9d8f-af2a304e4fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 20380), ('the', 929), ('and', 680), ('of', 625), ('to', 608), ('I', 523), ('a', 453), ('my', 444), ('in', 382), ('you', 361)]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5b354-d307-434a-8690-469390e66650",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3f4ee-5f92-474b-9c4a-2b3e958fcfe8",
   "metadata": {},
   "source": [
    "# Accumulator and Broadcast Variables in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb216be-585c-44ba-a725-a35272f94bf0",
   "metadata": {},
   "source": [
    "## Theory\n",
    "Functions that are passed to methods like map() or filter() can use variables defined in the global scope (i.e., the driver program).  \n",
    "Each task running on the cluster gets a new copy of each variable, and updates from these copies are not propagated back to the driver.  \n",
    "Spark automatically sends all variables referenced in closures to the worker nodes.  \n",
    "However, this can be inefficient because the default task launching mechanism is optimized for small task sizes. Moreover, the same variable may be used in multiple parallel operations, but Spark will send it separately for each operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22529da8-13cb-4a26-be36-dad78570602a",
   "metadata": {},
   "source": [
    "So there exist two common types of communication patterns that help prevent bottlenecks in the efficiency of a Spark application:\n",
    "1. aggregation of results\n",
    "2. broadcasts\n",
    "\n",
    "**Accumulators** provide a mechanism for aggregating values from worker nodes back to the driver program.   \n",
    "**Broadcast** variables allow the application to **efficiently** send large, read-only values to all worker nodes for use in RDD operations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781c239-6714-4a9c-a8ce-7ff241bcc1b8",
   "metadata": {},
   "source": [
    "### Broadcast Variables\n",
    "**Definition:** Broadcast variables are read-only shared variables that are cached and available on all nodes of a cluster.   \n",
    "Instead of sending this data with every task, Spark uses efficient broadcast algorithms to distribute broadcast variables.  \n",
    "They bring significant efficiency **enhancements when large, read-only lookup tables or even a large feature vector in an ML algorithm must be sent to all nodes.** (when to use)  \n",
    "Spark automatically sends all variables referenced in closures to the worker nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "618b1e9e-8b81-41ec-b43b-b39b874fce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following 'colors' list is a typical case of a lookup table\n",
    "colors = {\n",
    "    'blue': (0,0,255),\n",
    "    'green': (0,255,0),\n",
    "    'red': (255,0,0),\n",
    "    'yellow': (255,255,0),\n",
    "    'white': (255,255,255),\n",
    "    'black': (0,0,0)    \n",
    "}\n",
    "\n",
    "# I do not reference this 'bcolor' broadcast variable in my application; it just serves the whole operation of caching 'colors' so it can be efficiently broadcast \n",
    "# to all the nodes referencing it. \n",
    "bcolor = sc.broadcast(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cbd57c4-0f13-45d5-9f15-8403f4565a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    ('table', 'green'),\n",
    "    ('fridge', 'white'),\n",
    "    ('TV', 'black'),\n",
    "    ('book', 'yellow')\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fc67805-e462-4b49-a737-c441f137e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('table', (0, 255, 0)), ('fridge', (255, 255, 255)), ('TV', (0, 0, 0)), ('book', (255, 255, 0))]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda x: (x[0], colors[x[1]]))\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a71b9-4faf-491c-9b6e-48a6ba7b385d",
   "metadata": {},
   "source": [
    "## Accumulator Variables\n",
    "\n",
    "**Definition:** Accumulator variables are shared variables used to accumulate a **value of any numeric type over associative and commutative operations**. Custom accumulator types may also be defined.  \n",
    "One of **the most common uses** of accumulators is to count events that occur during job execution for debugging purposes.  \n",
    "**Tasks cannot read the accumulator** value; **only the driver program** can read it, **using its '.value()' method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01a9f6a0-c154-4dfb-ade1-6964cf9eae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of odds: 25\n"
     ]
    }
   ],
   "source": [
    "# First example - numeric summation\n",
    "acc = sc.accumulator(0)  # initial value is set to zero in this use case\n",
    "\n",
    "rdd = sc.parallelize([1,3,5,7,9])\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "\n",
    "print(f'Sum of odds: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "119a92e8-da8b-45c3-8bb7-e4d9f0557a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter concats: abcde\n"
     ]
    }
   ],
   "source": [
    "# Second example: string concatenation - this makes for a case where we define our own, custom accumulator variable (since by default accs only support numeric values)\n",
    "# For this reason, we will need to extend on the behavior (aka methods) of the default AccumulatorParam class !!! \n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class strAcc(AccumulatorParam):\n",
    "    def zero(self, _):  # initial value of the accumulator variable ! \n",
    "        return ''\n",
    "    def addInPlace(self, var, val):  # this defines how the `.add()` method will behave\n",
    "        return var + val\n",
    "\n",
    "accs = sc.accumulator('', strAcc())  \n",
    "rdd1 = sc.parallelize(['a','b','c','d','e'])\n",
    "rdd1.foreach((lambda x: accs.add(x)))\n",
    "\n",
    "print(f'Letter concats: {accs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c1bcb-116c-4dbb-876a-f63fac1bc442",
   "metadata": {},
   "source": [
    "## Partitioning and Repartitioning\n",
    "\n",
    "### Partitioning \n",
    "- **Local partitioning:** when running on local in standalone mode, Spark partitions data into the number of CPU cores or the value specified when SparkContext is created\n",
    "- **Cluster partitioning:** when running on HDFS cluster mode, spark creates one partition for each block of the file. The number of partitions will be equal to max(total number of cores on all executor nodes in a cluster, 2).\n",
    "\n",
    "### Repartitioning\n",
    "Repartition is **used to increase or decrease the RDD partitions**.\n",
    "\n",
    "### Coalesce \n",
    "**Coalesce** is used to only decrease the number of partitions **in an efficient way**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d2fa0-f4cb-4ff6-937a-65f81baf2dc4",
   "metadata": {},
   "source": [
    "### RDD Shuffling\n",
    "**Definition:** RDD Shuffling is a mechanism for redistributing or repartitioning data so that the data get grouped differently across partitions, across different executors, and even across machines. \n",
    "It is an expensive operation as it moves the data between executors or even between worker nodes in a cluster. This leads to heavy Disk I/O, it involves the costly operations of data serialization and deserialization, as well as Network I/O.  \n",
    "**Some of the operations that trigger shuffling** are 'repartition()', 'coalesce()', 'groupByKey()', 'reduceByKey()', 'cogroup()', 'join()'.  \n",
    "Shuffling is **NOT triggered by 'countByKey()'**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357be3c2-0351-4810-b7d6-20991ba8fdbd",
   "metadata": {},
   "source": [
    "# Working with Multiple Datasets in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984cb404-9e5a-49bb-ba23-a2325c3ded4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = sc.parallelize([\n",
    "    (\"index.html\", \"1.2.3.4\"),\n",
    "    (\"about.html\", \"3.4.5.6\"),\n",
    "    (\"index.html\", \"1.3.3.1\")\n",
    "])\n",
    "\n",
    "pageNames = sc.parallelize([\n",
    "    (\"index.html\", \"Home\"),\n",
    "    (\"about.html\", \"About\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3ee98-8a6d-47df-a4f5-cb6da1bd99ab",
   "metadata": {},
   "source": [
    "### RDD Join & Cogroup in PySpark\n",
    "Both .join() and .cogroup() are used for combining data from two RDDs, but they serve different purposes and behave differently in their output structure.  \n",
    "1. **join()**\n",
    "    - **Behavior:** Performs an inner join on two RDDs based on their keys.\n",
    "    - **Returns:** A new RDD where each key is associated with a tuple of values from both RDDs.\n",
    "    - **Structure:** (key, (value_from_rdd1, value_from_rdd2))\n",
    "    - **Ideal for:** One-to-one or one-to-many relationships where you need pairwise matching of elements, or relational-style lookups (e.g., mapping user visits to webpage names).\n",
    "\n",
    "\n",
    "2. **cogroup()**\n",
    "    - **Behavior:** Groups values from both RDDs by key but does not directly pair them like join().\n",
    "    - **Returns:** A new RDD where each key is associated with two result iterables:\n",
    "        - The first iterable contains values from RDD1.\n",
    "        - The second iterable contains values from RDD2.\n",
    "    - **Structure:** (key, (iterable_from_rdd1, iterable_from_rdd2))\n",
    "    - **Ideal for:** Many-to-many relationships or custom aggregation; cases where you need to group values before processing, rather than directly joining them; cases where you want to aggregate values before performing an operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b76c7ec-1e21-41a5-bd8d-3696a0cfd912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('about.html', ('3.4.5.6', 'About')),\n",
       " ('index.html', ('1.2.3.4', 'Home')),\n",
       " ('index.html', ('1.3.3.1', 'Home'))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RDD Join implementation\n",
    "\n",
    "visits.join(pageNames).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba90b81c-86f4-4013-ac66-d84d4c6639f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('about.html',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1229e6e00>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1229e6b60>)),\n",
       " ('index.html',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1229e54e0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1229e6c80>))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RDD Cogroup implementation\n",
    "\n",
    "visits.cogroup(pageNames).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be73a1d-d6f2-400d-83f5-98f3aba25beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('about.html', (['3.4.5.6'], ['About'])), ('index.html', (['1.2.3.4', '1.3.3.1'], ['Home']))]\n"
     ]
    }
   ],
   "source": [
    "# To better view the iterables matched to each key:\n",
    "result = visits.cogroup(pageNames).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cfb7d-f23e-4a98-9a76-035ecd35ece7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
