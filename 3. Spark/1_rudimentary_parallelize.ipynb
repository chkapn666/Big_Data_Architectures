{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1079f6f-e857-410e-8467-394c87474f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04bba5e-bd9e-4c84-8942-bf50e51499f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 12:30:51 WARN Utils: Your hostname, ChristoorossAir resolves to a loopback address: 127.0.0.1; using 192.168.1.18 instead (on interface en0)\n",
      "25/03/16 12:30:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 12:30:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/16 12:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 63922)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "maxNum = 10_000\n",
    "sc = SparkContext('local[1]', 'app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8744862e-2fcc-421d-bc1c-58ed071d064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(\n",
    "    range(maxNum),  # python range object\n",
    "    5   # we explicitly ask for 5 partitions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7388d5e3-6b25-44a2-872f-ed31b427fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Action: First Element: 0\n",
      "Action: sum, mean of values: 49995000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {0}\".format(rdd.getNumPartitions()))\n",
    "print(\"Action: First Element: {0}\".format(rdd.first()))\n",
    "print(\"Action: sum, mean of values: {0}\".format(rdd.sum(), rdd.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ddf210-3df8-45b8-bd82-515253f53dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: RDD first ten values: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(\"Action: RDD first ten values: \")\n",
    "rdd = rdd.filter(lambda x: x < 10)\n",
    "rdd.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d238587-fa27-4c15-8ee8-dc40ee553c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCollect = rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24423695-b1be-44f1-bc38-9ec40e2923c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: RDD converted to Array[Int], first ten values: \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 12:36:13 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/fp/b19fbw2j41z9_lyxq1cyd88h0000gn/T/blockmgr-3a6bd2a6-29bb-4b85-badd-8bb4349628e0. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/fp/b19fbw2j41z9_lyxq1cyd88h0000gn/T/blockmgr-3a6bd2a6-29bb-4b85-badd-8bb4349628e0\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action: RDD converted to Array[Int], first ten values: \")\n",
    "for n in rddCollect[:10]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6bfef-ed22-4792-98d1-e8824035d300",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37def0d9-5dd8-48b3-abb8-55d75ca42ae2",
   "metadata": {},
   "source": [
    "# wholeTextFiles parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fafaa2-f1ad-462b-bb4b-98810fbdfc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in multiple files and creating key-value pairs <=> file name-file content pairs\n",
    "# when using a wildcart, we have to use the FULL PATH \n",
    "rdd = sc.wholeTextFiles(\"file:///Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3. Spark/*.txt\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaddd914-b6f1-47ab-8649-14ae87f1be4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "print('Number of partitions: {0}'.format(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107ce8a2-309f-4257-86e9-6e848eeb7301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file:/Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3. Spark/text01.txt:\n",
      "Text file 1\n",
      "Data 1, 1\n",
      "Data 1, 2\n",
      "\n",
      "\n",
      "\n",
      "file:/Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3. Spark/text02.txt:\n",
      "Text file 2\n",
      "Data 2, 1\n",
      "Data 2, 2\n",
      "Data 2, 3\n",
      "\n",
      "\n",
      "file:/Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3. Spark/text03.txt:\n",
      "Text file 3\n",
      "Data 3, 1\n",
      "Data 3, 2\n",
      "Data 3, 3\n",
      "\n",
      "\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rdd.foreach(lambda f: print('{0}:\\n{1}\\n\\n'.format(f[0],f[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e6fe1-bdd8-4517-a83a-74dabfc27c6f",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbeaa31-b13a-45b9-be1b-f7b252eed80c",
   "metadata": {},
   "source": [
    "# Reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef095e-181c-4818-8147-2664bd63fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('file:///Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/3. Spark/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f577d0-cb76-4d82-90b9-9bd246f19f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emp1,100,3000\n",
      "Emp2,120,2900\n",
      "Emp3,110,3100\n",
      "Emp4,110,3300\n",
      "Emp5,90,2800\n",
      "Emp6,120,3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 12:46:46 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/fp/b19fbw2j41z9_lyxq1cyd88h0000gn/T/blockmgr-321a3a6d-5984-4e70-bea7-6481c24badf9. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/fp/b19fbw2j41z9_lyxq1cyd88h0000gn/T/blockmgr-321a3a6d-5984-4e70-bea7-6481c24badf9\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Splitting must be done manually !!!\n",
    "# Headers must be removed manually !!! \n",
    "\n",
    "rdd1 = rdd.map(lambda x: x.split(','))\n",
    "for r in rdd.collect():\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
