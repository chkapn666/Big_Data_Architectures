{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6c2004-c2cd-4b69-9f5e-2e4b50311a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9b5e32-d1bb-4835-8666-3ed5901c934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: option\n",
      "25/03/16 20:53:09 WARN Utils: Your hostname, ChristoorossAir resolves to a loopback address: 127.0.0.1; using 192.168.1.18 instead (on interface en0)\n",
      "25/03/16 20:53:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 20:53:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57786)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"app\") \\\n",
    "            .config(\"option\", \"value\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20c69e0-4a69-4f10-bc0f-6d84eb09f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkDF from sparkContext parallelized container\n",
    "\n",
    "# 1st way to do this\n",
    "rdd = spark.sparkContext.parallelize([1,3,5,7,9])\n",
    "df = rdd.map(lambda x: (x, )).toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88066bcb-a29f-4bfa-8fd1-aae9dc841e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: integer (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    2|\n",
      "|    4|\n",
      "|    6|\n",
      "|    8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2nd way to do this\n",
    "\n",
    "df1 = spark.createDataFrame([0,2,4,6,8], IntegerType())\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72839821-a640-4afa-9b27-e3422bed49f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      "\n",
      "+----------+---------+\n",
      "|First Name|Last Name|\n",
      "+----------+---------+\n",
      "|      John|    Smith|\n",
      "|     Maria|    Jones|\n",
      "|     Peter|  Gabriel|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# improving the 1st way with also specifying the column names\n",
    "\n",
    "rdd2 = spark.sparkContext.parallelize([\n",
    "    ('John', 'Smith'),\n",
    "    ('Maria', 'Jones'),\n",
    "    ('Peter', 'Gabriel')\n",
    "])\n",
    "\n",
    "df2 = rdd2.toDF(['First Name', 'Last Name'])\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e43e15-047e-4e7a-8bb9-daa2c96961ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+-----+----+----+\n",
      "|  _c0| _c1| _c2|\n",
      "+-----+----+----+\n",
      "|Peter|3000| 200|\n",
      "|Helen|3100| 180|\n",
      "|Maria|2900| 250|\n",
      "| John|3600| 300|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3rd way - reading from a file \n",
    "import os\n",
    "\n",
    "# reading from a file without a header line\n",
    "df = spark.read.csv('file:///' + os.getcwd() + '/salaries.csv')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c1bc79-33e2-456c-ac6e-bca0d137603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "| Name|Monthly|Bonus|\n",
      "+-----+-------+-----+\n",
      "|Peter|   3000|  200|\n",
      "|Helen|   3100|  180|\n",
      "|Maria|   2900|  250|\n",
      "| John|   3600|  300|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If i wanted to manually assign the non-existent names\n",
    "df = df.toDF(\"Name\", \"Monthly\", \"Bonus\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67869e9-4cb8-4584-8bc1-5a0b8b1cf6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Monthly: string (nullable = true)\n",
      " |-- Bonus: string (nullable = true)\n",
      "\n",
      "+-----+-------+-----+\n",
      "| Name|Monthly|Bonus|\n",
      "+-----+-------+-----+\n",
      "|Peter|   3000|  200|\n",
      "|Helen|   3100|  180|\n",
      "|Maria|   2900|  250|\n",
      "| John|   3600|  300|\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading from a file with a header line\n",
    "df1 = spark.read.csv('file:///' + os.getcwd() + '/salariesH.csv', header=True)\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e81402f-75a1-420a-9384-3562c9c1ed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+-----------+\n",
      "|             message|                 ts|       user|\n",
      "+--------------------+-------------------+-----------+\n",
      "|        Today's news|2020-09-02T10:32:41|    anon-gr|\n",
      "|Coronavirus sad u...|2020-09-01T22:42:04|generationx|\n",
      "|   It is a sunny day|2020-08-31T03:22:11|    anon-gr|\n",
      "|Back from summer ...|2020-08-29T20:57:44|generationx|\n",
      "+--------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4th way - reading from a json file \n",
    "\n",
    "df2 = spark.read.option(\"multiline\", \"true\").json('file:///' + os.getcwd() + '/tweets.json')\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afe76328-36c4-45bc-ae04-de2007167bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th way - interacting with a database \n",
    "import mysql.connector  # module mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f20017e-b384-4827-931c-ea0638eac446",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "    user='ckapsalis',\n",
    "    password='ckapsalis',\n",
    "    host='127.0.0.1'\n",
    ");\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bf949df-c80e-44c2-9daf-ede35c014530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a db \n",
    "\n",
    "#WARNING: only one sql command per 'cursor.execute()'\n",
    "\n",
    "cursor.execute(\"DROP DATABASE IF EXISTS suppliersProducts;\")\n",
    "cursor.execute(\"CREATE DATABASE suppliersProducts;\")\n",
    "cursor.execute(\"USE suppliersProducts;\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE supplier (\n",
    "        id CHAR(20) NOT NULL primary key,\n",
    "        name CHAR(20),\n",
    "        status INT,\n",
    "        city CHAR(20)\n",
    "    );\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE product (\n",
    "        id CHAR(20) NOT NULL primary key,\n",
    "        name CHAR(20),\n",
    "        color CHAR(20),\n",
    "        weight FLOAT,\n",
    "        city CHAR(20)\n",
    "    );\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE supplierProduct (\n",
    "        sid CHAR(20),\n",
    "        FOREIGN KEY(sid) REFERENCES supplier(id),\n",
    "        pid CHAR(20),\n",
    "        FOREIGN KEY (pid) REFERENCES product(id),\n",
    "        qty INT,\n",
    "        PRIMARY KEY(sid,pid)\n",
    "    );\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1557ec07-a702-4437-9401-c564ac5270e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding data into the created dataset\n",
    "\n",
    "cursor.execute(\"INSERT INTO supplier(ID, NAME, STATUS, CITY) VALUES ('S1', 'Smith', 20, 'London');\")\n",
    "cursor.execute(\"INSERT INTO supplier(ID, NAME, STATUS, CITY) VALUES ('S2', 'Jones', 10, 'Paris');\")\n",
    "cursor.execute(\"INSERT INTO supplier(ID, NAME, STATUS, CITY) VALUES ('S3', 'Blake', 30, 'Paris');\")\n",
    "cursor.execute(\"INSERT INTO supplier(ID, NAME, STATUS, CITY) VALUES ('S4', 'Clark', 20, 'London');\")\n",
    "cursor.execute(\"INSERT INTO supplier(ID, NAME, STATUS, CITY) VALUES ('S5', 'Adams', 30, 'Athens');\")\n",
    "\n",
    "cursor.execute(\"INSERT INTO product(ID, NAME, COLOR, WEIGHT, CITY) VALUE ('P1', 'Nut', 'Red', 12.0, 'London');\")\n",
    "cursor.execute(\"INSERT INTO product(ID, NAME, COLOR, WEIGHT, CITY) VALUE ('P2', 'Bolt', 'Green', 17.0, 'Paris');\")\n",
    "cursor.execute(\"INSERT INTO product(ID, NAME, COLOR, WEIGHT, CITY) VALUE ('P3', 'Screw', 'Blue', 17.0, 'Oslo');\")\n",
    "cursor.execute(\"INSERT INTO product(ID, NAME, COLOR, WEIGHT, CITY) VALUE ('P4', 'Screw', 'Red', 14.0, 'London');\")\n",
    "cursor.execute(\"INSERT INTO product(ID, NAME, COLOR, WEIGHT, CITY) VALUE ('P5', 'Cam', 'Blue', 12.0, 'Paris');\")\n",
    "cursor.execute(\"INSERT INTO product(ID, NAME, COLOR, WEIGHT, CITY) VALUE ('P6', 'Cog', 'Red', 19.0, 'London');\")\n",
    "\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S1', 'P1', 200);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S2', 'P3', 400);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S2', 'P5', 100);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S3', 'P3', 200);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S3', 'P4', 500);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S4', 'P6', 300);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S5', 'P2', 200);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S5', 'P5', 500);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S5', 'P6', 200);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S5', 'P1', 100);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S5', 'P3', 200);\")\n",
    "cursor.execute(\"INSERT INTO supplierProduct(SID, PID, QTY) VALUE ('S5', 'P4', 800);\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c416426-93e4-49e8-a23f-316a63048ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5aebf3-9a2f-4e75-8783-f37b23c53407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 21:21:55 WARN Utils: Your hostname, ChristoorossAir resolves to a loopback address: 127.0.0.1; using 192.168.1.18 instead (on interface en0)\n",
      "25/03/16 21:21:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/16 21:21:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe from this database\n",
    "# !!! We need to use JDBC (Java Database Connectivity) to connect Spark to our MySQL database and load a table as a Spark DataFrame.\n",
    "# I have downloaded the platform-agnostic version and unzipped to /Users/chkapsalis/Documents/mysql-connector-j-9.2.0\n",
    "\n",
    "# I need to restart the kernel so as to be able to create more SparkSessions\n",
    "# I also need to start the mysql process in the background (terminal -> \"mysql.server start\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .config(\"spark.jars\", \"/Users/chkapsalis/Documents/mysql-connector-j-9.2.0/mysql-connector-j-9.2.0.jar\") \\\n",
    "            .appName(\"app\") \\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aff73b0-4fb1-4606-88a6-35aed1cd2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+------+--------------------+\n",
      "|                  id|                name|               color|weight|                city|\n",
      "+--------------------+--------------------+--------------------+------+--------------------+\n",
      "|P1                  |Nut                 |Red                 |  12.0|London              |\n",
      "|P2                  |Bolt                |Green               |  17.0|Paris               |\n",
      "|P3                  |Screw               |Blue                |  17.0|Oslo                |\n",
      "|P4                  |Screw               |Red                 |  14.0|London              |\n",
      "|P5                  |Cam                 |Blue                |  12.0|Paris               |\n",
      "|P6                  |Cog                 |Red                 |  19.0|London              |\n",
      "+--------------------+--------------------+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306/suppliersproducts\") \\\n",
    "        .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "        .option(\"dbtable\", \"product\") \\\n",
    "        .option(\"user\", \"ckapsalis\") \\\n",
    "        .option(\"password\", \"ckapsalis\") \\\n",
    "        .load()\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2d65a9-cb7c-46e1-b850-a92cc1d4d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age\n",
      "0   Nick   26\n",
      "1  Helen   28\n",
      "2   Mary   30\n",
      "3   John   29\n"
     ]
    }
   ],
   "source": [
    "# 5th way - from pandas dataframe \n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "data = [\n",
    "    ['Nick', 26],\n",
    "    ['Helen', 28],\n",
    "    ['Mary', 30],\n",
    "    ['John', 29]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Name\", \"Age\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b74b96a-ac54-4031-b4da-b673de8de586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 21:26:35 WARN Utils: Your hostname, ChristoorossAir resolves to a loopback address: 127.0.0.1; using 192.168.1.18 instead (on interface en0)\n",
      "25/03/16 21:26:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 21:26:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| Nick| 26|\n",
      "|Helen| 28|\n",
      "| Mary| 30|\n",
      "| John| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"app\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n",
    "sdf = spark.createDataFrame(df)\n",
    "sdf.printSchema()\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65e73c-4aca-4cbe-87fa-df5bf3b1b83a",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8b374-46c5-4b0a-93e8-2de364d60e06",
   "metadata": {},
   "source": [
    "## Spark DataFrame Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccc52f2-e44a-4936-b57d-a40a13fd8504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b27639-0f0a-4aa8-a39f-a38fb8a5c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "| Nick|\n",
      "|Helen|\n",
      "| Mary|\n",
      "| John|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" column\n",
    "sdf.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fe8f46-7ca3-4acb-9b9d-89aec6b6fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| Name|(Age + 1)|\n",
      "+-----+---------+\n",
      "| Nick|       27|\n",
      "|Helen|       29|\n",
      "| Mary|       31|\n",
      "| John|       30|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting Columns\n",
    "sdf.select(sdf[\"Name\"], sdf[\"Age\"]+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd198263-de0f-4e64-83e2-eb91a2536c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Helen| 28|\n",
      "| Mary| 30|\n",
      "| John| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering Columns\n",
    "sdf.filter(sdf[\"Age\"] >= 28).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27bacfb-6446-4655-9fb6-4450d54546f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 26|    1|\n",
      "| 29|    1|\n",
      "| 28|    1|\n",
      "| 30|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping\n",
    "sdf.groupBy(\"Age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f90ea-15a4-426d-a503-d59a43d1a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
