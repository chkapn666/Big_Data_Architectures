{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "os.chdir('/Users/chkapsalis/Documents/GitHub/Big_Data_Architectures/Assignments/my_assignment_3')\n",
    "\n",
    "# For some reason i need to run this every time in order to get it work\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home\" \n",
    "\n",
    "# the .config(\"option\", \"value\") arguments allow us to perform refined file I/O\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"app\") \\\n",
    "            .config(\"option\", \"value\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'DATETIME,LAT,LONG,DEPTH,MAGNITUDE'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATETIME: string (nullable = true)\n",
      " |-- LAT: string (nullable = true)\n",
      " |-- LONG: string (nullable = true)\n",
      " |-- DEPTH: string (nullable = true)\n",
      " |-- MAGNITUDE: string (nullable = true)\n",
      "\n",
      "+----------------+-----+-----+-----+---------+\n",
      "|        DATETIME|  LAT| LONG|DEPTH|MAGNITUDE|\n",
      "+----------------+-----+-----+-----+---------+\n",
      "|01/07/1965 10:22| 36.5| 26.5|   10|      5.3|\n",
      "|01/10/1965 08:02|39.25|22.25|   10|      4.9|\n",
      "|01/12/1965 17:26|   37|   22|   10|        4|\n",
      "| 1/15/1965 14:56|36.75|21.75|   10|      4.5|\n",
      "|03/09/1965 19:16|   39|   24|   10|      4.2|\n",
      "|03/09/1965 20:14|   39|   24|   10|      4.4|\n",
      "|03/10/1965 00:04|   39|   24|   10|      4.8|\n",
      "|03/10/1965 21:50|   39|   24|   10|      4.8|\n",
      "| 3/31/1965 12:01| 38.5|21.75|   10|      5.1|\n",
      "|04/03/1965 05:19|38.11| 23.7|   10|      4.6|\n",
      "| 5/15/1965 13:34| 37.5|21.75|   10|        4|\n",
      "|07/06/1965 06:22| 38.4| 22.3|   10|      4.1|\n",
      "|07/07/1965 23:26| 38.4| 22.2|   10|      3.8|\n",
      "| 7/16/1965 13:54| 38.5|23.25|   10|      3.3|\n",
      "|  8/14/1965 4:47| 38.5|   22|   10|      3.6|\n",
      "|  8/26/1965 8:59| 38.5|22.25|   10|      3.4|\n",
      "| 8/31/1965 10:51| 37.5| 21.5|   10|      3.9|\n",
      "|09/05/1965 16:34|35.25|25.25|   10|      4.1|\n",
      "|09/05/1965 20:26| 38.5| 24.5|   10|      3.2|\n",
      "|09/06/1965 23:06| 39.3| 21.9|   10|      3.6|\n",
      "+----------------+-----+-----+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quakes = spark.read.csv('file:///' + os.getcwd() + '/earthquakes.csv', header=True)\n",
    "quakes.printSchema()\n",
    "quakes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|YEAR|MAGNITUDE|\n",
      "+----+---------+\n",
      "|1968|      6.7|\n",
      "|2020|      6.7|\n",
      "|1983|      6.6|\n",
      "|2008|      6.5|\n",
      "|1982|      6.4|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "# This implementation is problematic due to how spark+java handle very old dates - i get NULL values due to\n",
    "# overflow issues when i try to print out dates older than 1970\n",
    "quakes_parsed = quakes.withColumn('YEAR', split('DATETIME', '[ /]')[2]) \\\n",
    "                    .withColumn('MAGNITUDE', col(\"MAGNITUDE\").cast(\"float\"))  # MAGNITUDE is initially taken in as string!!!\n",
    "                    # !!! I MUST ALWAYS PRINT THE SCHEMA AT FIRST !!! #\n",
    "quakes_parsed \\\n",
    "    .orderBy(\"MAGNITUDE\", ascending=False) \\\n",
    "      .select(\n",
    "          col(\"YEAR\"),\n",
    "          col(\"MAGNITUDE\")\n",
    "      ) \\\n",
    "      .limit(5) \\\n",
    "      .show()\n",
    "\n",
    "\n",
    "# quakes_parsed = quakes.withColumn(\\\"parsed_dt\\\", to_timestamp(\\\"DATETIME\\\", \\\"dd/MM/yyyy HH:mm\\\"))\n",
    "# quakes_parsed \\\n",
    "#    .orderBy(\\\"MAGNITUDE\\\", ascending=False) \\\n",
    "#      .select(\n",
    "#          to_date(col(\\\"parsed_dt\\\")).alias(\\\"DATE\\\"),\n",
    "#          date_format(col(\\\"parsed_dt\\\"), \\\"HH:mm\\\").alias(\\\"TIME\\\"),\n",
    "#          col(\\\"YEAR\\\"),\n",
    "#          col(\\\"MAGNITUDE\\\")\n",
    "#      ) \\\n",
    "#      .limit(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+\n",
      "|      DATE| TIME|MAGNITUDE|\n",
      "+----------+-----+---------+\n",
      "|2/19/1968 | 2:45|      6.7|\n",
      "|10/30/2020|11:51|      6.7|\n",
      "|08/06/1983|15:43|      6.6|\n",
      "|06/08/2008|12:25|      6.5|\n",
      "|1/18/1982 | 9:27|      6.4|\n",
      "|01/08/2006|11:34|      6.4|\n",
      "|2/24/1981 | 0:53|      6.3|\n",
      "|12/19/1981|14:10|      6.3|\n",
      "|5/24/2014 |  :25|      6.3|\n",
      "|7/15/2008 |  :26|      6.2|\n",
      "+----------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quakes.select(\n",
    "    substring(\"DATETIME\", 1, 10).alias(\"DATE\"),\n",
    "    substring(\"DATETIME\", 12, 5).alias(\"TIME\"),\n",
    "    col(\"MAGNITUDE\")\n",
    ").orderBy(col(\"MAGNITUDE\").desc()) \\\n",
    " .limit(10) \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|YEAR|count|\n",
      "+----+-----+\n",
      "|2011| 5624|\n",
      "|2012| 7036|\n",
      "|2013| 8056|\n",
      "|2014|10172|\n",
      "|2015| 6989|\n",
      "|2016| 5523|\n",
      "|2017| 4468|\n",
      "|2018| 3431|\n",
      "|2019| 5185|\n",
      "|2020| 4178|\n",
      "|2021| 9042|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# Adding a properly parsed datetime column to the current dataframe\n",
    "quakes_with_year = quakes.withColumn(\"parsed_dt\", to_timestamp(\"DATETIME\", \"dd/MM/yyyy HH:mm\")) \\\n",
    "                        .withColumn('MAGNITUDE', col(\"MAGNITUDE\").cast(\"float\")) \n",
    "\n",
    "quakes_with_year.select(\n",
    "            date_format(col(\"parsed_dt\"), \"yyyy\").alias(\"YEAR\")\n",
    "        ) \\\n",
    "        .groupBy(\"YEAR\") \\\n",
    "        .count() \\\n",
    "        .sort(\"YEAR\", ascending=True) \\\n",
    "        .filter(col(\"YEAR\") > 2010) \\\n",
    "        .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chkapsalis/.pyenv/versions/base/lib/python3.10/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "quakes_with_year.registerTempTable('quakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = spark.sql(\"\"\"\n",
    "   SELECT \n",
    "      YEAR(parsed_dt),\n",
    "      MIN(MAGNITUDE) AS min_magn,\n",
    "      MAX(MAGNITUDE) AS max_magn,\n",
    "      AVG(MAGNITUDE) AS avg_magn\n",
    "   FROM quakes\n",
    "   WHERE YEAR(parsed_dt) BETWEEN 2010 AND 2020\n",
    "   GROUP BY YEAR(parsed_dt)\n",
    "   ORDER BY YEAR(parsed_dt) ASC\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+--------+------------------+\n",
      "|year(parsed_dt)|min_magn|max_magn|          avg_magn|\n",
      "+---------------+--------+--------+------------------+\n",
      "|           2010|     0.8|     5.1|2.7314732111857407|\n",
      "|           2011|     0.2|     6.2| 1.743847792459437|\n",
      "|           2012|     0.2|     5.0|1.6250284232046708|\n",
      "|           2013|     0.2|     5.8|1.7396350512974696|\n",
      "|           2014|     0.2|     5.7|1.8499508410870935|\n",
      "|           2015|     0.3|     5.3|1.7466304144227875|\n",
      "|           2016|     0.1|     4.9|1.8376244756715379|\n",
      "|           2017|     0.2|     6.1|1.8582363438124676|\n",
      "|           2018|     0.1|     4.8|1.9132905825972557|\n",
      "|           2019|     0.5|     5.4|1.9195178357476554|\n",
      "|           2020|     0.4|     5.2|1.9605313521672116|\n",
      "+---------------+--------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "r3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+------------------+\n",
      "|year|min_magn|max_magn|          avg_magn|\n",
      "+----+--------+--------+------------------+\n",
      "|2010|     0.8|     5.1|2.7314732111857407|\n",
      "|2011|     0.2|     6.2| 1.743847792459437|\n",
      "|2012|     0.2|     5.0|1.6250284232046708|\n",
      "|2013|     0.2|     5.8|1.7396350512974696|\n",
      "|2014|     0.2|     5.7|1.8499508410870935|\n",
      "|2015|     0.3|     5.3|1.7466304144227875|\n",
      "|2016|     0.1|     4.9|1.8376244756715379|\n",
      "|2017|     0.2|     6.1|1.8582363438124676|\n",
      "|2018|     0.1|     4.8|1.9132905825972557|\n",
      "|2019|     0.5|     5.4|1.9195178357476554|\n",
      "|2020|     0.4|     5.2|1.9605313521672116|\n",
      "+----+--------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE SOLUTION TO QUESTION 3, WITHOUT THE USE OF SPARK SQL - PURE SPARK DATAFRAME SOLUTION\n",
    "quakes_2010_2020 = quakes \\\n",
    "    .withColumn(\"parsed_dt\", to_timestamp(\"DATETIME\", \"dd/MM/yyyy HH:mm\")) \\\n",
    "    .withColumn(\"year\", year(col(\"parsed_dt\")).cast(IntegerType())) \\\n",
    "    .withColumn(\"MAGNITUDE\", col(\"MAGNITUDE\").cast(\"float\")) \\\n",
    "    .filter((col(\"year\") >= 2010) & (col(\"year\") <= 2020)) \\\n",
    "    .groupBy(\"year\") \\\n",
    "    .agg(\n",
    "        min(\"MAGNITUDE\").alias(\"min_magn\"),\n",
    "        max(\"MAGNITUDE\").alias(\"max_magn\"),\n",
    "        avg(\"MAGNITUDE\").alias(\"avg_magn\")\n",
    "    ) \\\n",
    "    .orderBy(\"year\")\n",
    "\n",
    "quakes_2010_2020.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "r4 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATETIME,\n",
    "        MAGNITUDE\n",
    "    FROM quakes\n",
    "    WHERE (LAT BETWEEN 37.5 AND 39.0) AND (LONG BETWEEN 23.35 AND 23.55)\n",
    "    ORDER BY MAGNITUDE DESC\n",
    "    LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|        DATETIME|MAGNITUDE|\n",
      "+----------------+---------+\n",
      "|06/09/2015 01:09|      5.3|\n",
      "|11/17/2014 23:05|      5.2|\n",
      "|11/17/2014 23:09|      5.2|\n",
      "| 7/19/2019 11:13|      5.1|\n",
      "| 10/14/2008 2:09|      4.7|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|        DATETIME|MAGNITUDE|\n",
      "+----------------+---------+\n",
      "|06/09/2015 01:09|      5.3|\n",
      "|11/17/2014 23:05|      5.2|\n",
      "|11/17/2014 23:09|      5.2|\n",
      "| 7/19/2019 11:13|      5.1|\n",
      "| 10/14/2008 2:09|      4.7|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alt_q4 = quakes \\\n",
    "    .withColumn(\"MAGNITUDE\", col(\"MAGNITUDE\").cast(\"float\")) \\\n",
    "    .withColumn(\"LAT\", col(\"LAT\").cast(\"float\")) \\\n",
    "    .withColumn(\"LONG\", col(\"LONG\").cast(\"float\")) \\\n",
    "    .filter((col(\"LAT\") >= 37.5) & (col(\"LAT\") <= 39) & (col(\"LONG\") >= 23.35) & (col(\"LONG\") <= 23.55)) \\\n",
    "    .select(\n",
    "        col(\"DATETIME\"),\n",
    "        col(\"MAGNITUDE\"),\n",
    "    ) \\\n",
    "    .sort(\"MAGNITUDE\", ascending=False) \\\n",
    "    .limit(5)\n",
    "alt_q4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
